# ebury-challenge

This challenge involves building a data pipeline to extract customer transaction data from a CSV filesystem. The challenge requires the use of on-premises resources, and the data platform stack consists of the following tools. In addition to extracting data, they were cleaned in the clean layer, modeled in the mart layer, and aggregated in the analytics layer.

- **Infra**: Docker
- **Storage**: Postgres
- **Ingestion**: dlthub
- **Transformation**: dbt
- **Orchestrator**: Airflow

# Repository Structure
```bash
├── dags  
│   └── ebury_dag.py  
├── data  
│   └── customer_transactions.csv  
├── dbt  
│   ├── dbt_project.yml  
│   ├── macros  
│   │   └── generate_schema_name.sql  
│   ├── models  
│   │   ├── aggregates  
│   │   │   ├── agg_monthly_transactions.sql  
│   │   │   └── agg_monthly_transactions.yml  
│   │   ├── clean  
│   │   │   ├── clean_customer_transactions.sql  
│   │   │   └── sources.yml  
│   │   └── marts  
│   │       ├── dim_customers.sql  
│   │       ├── dim_customers.yml  
│   │       ├── fact_transactions.sql  
│   │       └── fact_transactions.yml  
│   ├── profiles.yml  
│   └── tests  
│       ├── test_agg_monthly_transactions.sql  
│       ├── test_dim_customers.sql  
│       └── test_fact_transactions.sql  
├── dlthub  
│   ├── ingestion.py  
├── docker-compose.yaml  
├── Dockerfile  
├── include  
│   ├── constants.py  
│   └── utils.py  
├── poetry.lock  
├── pyproject.toml  
├── README.md  
└── scripts  
    ├── project_config.sh  
    └── setup.sh  
```
- **dags**:
  - `ebury_dag.py`: It is responsible for scheduling and managing the execution of tasks such as data extraction, transformation, and loading.

- **data**:
  - `customer_transactions.csv`: A raw data file containing customer transaction records. This file serves as the data source to be processed in the pipeline.

- **dbt** (Data Build Tool):
  - `dbt_project.yml`: The configuration file for the dbt project, which defines project-specific settings such as data warehouse connections, materializations, and other configurations.
  - **macros**:
    - `generate_schema_name.sql`: A SQL file containing custom macros used within dbt models. These macros assist in automating tasks, such as dynamically generating schema names.
  - **models**:
    - **aggregates**:
      - `agg_monthly_transactions.sql`: A SQL model for aggregating transaction data on a monthly basis.
      - `agg_monthly_transactions.yml`: A YAML configuration file for the `agg_monthly_transactions` model.
    - **clean**:
      - `clean_customer_transactions.sql`: A SQL model for cleaning and transforming raw customer transaction data.
      - `sources.yml`: Defines the source tables or files from which dbt models are built, ensuring dbt knows where to pull data from.
    - **marts**:
      - `dim_customers.sql`: A SQL model for creating the customer dimension table.
      - `dim_customers.yml`: A YAML configuration file for the `dim_customers` table.
      - `fact_transactions.sql`: A SQL model for creating the fact table of transactions.
      - `fact_transactions.yml`: A YAML configuration file for the `fact_transactions` table.
  - `profiles.yml`: A configuration file for dbt profiles, which defines the database connection settings required to run dbt models.
  - **tests**:
    - `test_agg_monthly_transactions.sql`: A SQL test file to validate the correctness of the monthly transaction aggregation model.
    - `test_dim_customers.sql`: A SQL test file to ensure the customer dimension model is correct.
    - `test_fact_transactions.sql`: A SQL test file to validate the transactions fact model.

- **dlthub**:
  - `ingestion.py`: A Python script responsible for ingesting data, such as extracting and loading customer transaction data into the data pipeline for processing.

- **docker-compose.yaml**: A Docker Compose file that defines and runs multi-container Docker applications. It likely configures the environment for the data pipeline, including services such as Airflo and Storage.

- **Dockerfile**: Instructions for building a Docker image.

- **include**:
  - `constants.py`: A Python file containing constant values used across the project.
  - `utils.py`: A Python file containing utility functions that provide reusable code for different tasks in the project.

- **poetry.lock**: A lock file generated by Poetry, containing specific versions of the dependencies used in the project.

- **pyproject.toml**: The configuration file for Poetry.

- **README.md**: A documentation file that provides an overview of the project, installation instructions, usage guidelines, and other relevant details for users and developers.

- **scripts**:
  - `project_config.sh`: A shell script for configuring the project's environment, such as setting environment variables and configuring paths.
  - `setup.sh`: A shell script for setting up the environment and dependencies.


## How to run the Project

In the **scripts** folder, an automation has been created to bring up all the infrastructure, so execute the following command:

```bash
chmod +x ./scripts/*
scripts/setup.sh start_airflow
```

To stop the execution of the project, run:
```bash
scripts/setup.sh stop_airflow
```

If the above commands do not work, execute these:

```bash
source scripts/project_config.sh
docker build -t $IMAGE_URI .
docker-compose up -d
```

To shut down Airflow, run:
```bash
docker-compose down
```

If everything works correctly, open Airflow at **http://localhost:8080** on your host machine to access Airflow. Upon arrival, you will be able to view the `daily_extraction_customer_transactions_data` DAG. This DAG represents the workflow for addressing the challenge, as illustrated in the image below.

![Ebury Challenge Pipeline](docs/airflow_task.png)
